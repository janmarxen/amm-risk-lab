{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2c1408",
   "metadata": {
    "id": "9f2c1408"
   },
   "source": [
    "# PLV Model Prototype 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07004618",
   "metadata": {
    "id": "07004618"
   },
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running in a fresh environment)\n",
    "# !pip install requests pandas\n",
    "# !pip install statsmodels\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba2d7d",
   "metadata": {
    "id": "25ba2d7d"
   },
   "source": [
    "## Fetch Data from Subgraph API\n",
    "\n",
    "Send the GraphQL query to the subgraph API endpoint and fetch the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5608d9c",
   "metadata": {
    "id": "c5608d9c"
   },
   "outputs": [],
   "source": [
    "def date_to_unix(date_str):\n",
    "    \"\"\"Convert a date string in 'YYYY-MM-DD' format to a Unix timestamp.\"\"\"\n",
    "    dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    return int(dt.timestamp())\n",
    "\n",
    "def make_price_query(pool_address, start_ts, end_ts):\n",
    "    \"\"\"Create a GraphQL query string for fetching price, volume, and liquidity of a Uniswap V3 pool.\"\"\"\n",
    "    return f\"\"\"\n",
    "    {{\n",
    "      poolHourDatas(first: 1000, where: {{pool: \\\"{pool_address}\\\", periodStartUnix_gte: {start_ts}, periodStartUnix_lte: {end_ts}}}, orderBy: periodStartUnix, orderDirection: asc) {{\n",
    "        periodStartUnix\n",
    "        token0Price\n",
    "        volumeUSD\n",
    "        liquidity\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "def fetch_pool_hourly_data(api_key, subgraph_id, pool_address, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch hourly price (token0Price as 'price'), volumeUSD, liquidity, and datetimes for a Uniswap V3 pool from the subgraph.\n",
    "    Dates should be in 'YYYY-MM-DD' format.\n",
    "    Returns a pandas DataFrame with columns: periodStartUnix, price, volumeUSD, liquidity\n",
    "    \"\"\"\n",
    "    start_ts = date_to_unix(start_date)\n",
    "    end_ts = date_to_unix(end_date)\n",
    "    all_data = []\n",
    "    last_ts = start_ts\n",
    "    while True:\n",
    "        graphql_query = make_price_query(pool_address, last_ts, end_ts)\n",
    "        payload = {\n",
    "            \"query\": graphql_query,\n",
    "            \"operationName\": \"Subgraphs\",\n",
    "            \"variables\": {}\n",
    "        }\n",
    "        url = f\"https://gateway.thegraph.com/api/subgraphs/id/{subgraph_id}\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Query failed with status code {response.status_code}: {response.text}\")\n",
    "        result = response.json()\n",
    "        hour_data = result['data']['poolHourDatas']\n",
    "        if not hour_data:\n",
    "            break\n",
    "        all_data.extend(hour_data)\n",
    "        if len(hour_data) < 1000:\n",
    "            break\n",
    "        # Update last_ts to one after the last returned timestamp to avoid overlap\n",
    "        last_ts = int(hour_data[-1]['periodStartUnix']) + 1\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    if not df.empty:\n",
    "        df['periodStartUnix'] = pd.to_datetime(df['periodStartUnix'], unit='s')\n",
    "        df = df.rename(columns={\"token0Price\": \"price\"})\n",
    "        df['price'] = pd.to_numeric(df['price'], errors='coerce').astype(float)\n",
    "        df['volumeUSD'] = pd.to_numeric(df['volumeUSD'], errors='coerce').astype(float)\n",
    "        df['liquidity'] = pd.to_numeric(df['liquidity'], errors='coerce').astype(float)\n",
    "    return df\n",
    "\n",
    "def fetch_all_pool_addresses(api_key, subgraph_id, pool_tier=None, first=1000):\n",
    "    \"\"\"\n",
    "    Fetch all pool addresses from a Uniswap subgraph with optional filtering by pool_tier.\n",
    "    - pool_tier: e.g., 'LOW', 'MEDIUM', 'HIGH', or None (no filter)\n",
    "    Returns a list of pool addresses.\n",
    "    \"\"\"\n",
    "    pool_addresses = []\n",
    "    skip = 0\n",
    "    while True:\n",
    "        filters = []\n",
    "        if pool_tier:\n",
    "            filters.append(f'tier: \"{pool_tier.upper()}\"')\n",
    "        where_clause = (', '.join(filters))\n",
    "        where_str = f'where: {{{where_clause}}}' if where_clause else ''\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "          pools(first: {first}, skip: {skip} {',' if where_str else ''}{where_str}) {{\n",
    "            id\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"query\": query,\n",
    "            \"operationName\": \"Subgraphs\",\n",
    "            \"variables\": {}\n",
    "        }\n",
    "        url = f\"https://gateway.thegraph.com/api/subgraphs/id/{subgraph_id}\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Query failed with status code {response.status_code}: {response.text}\")\n",
    "        result = response.json()\n",
    "        pools = result['data']['pools']\n",
    "        if not pools:\n",
    "            break\n",
    "        pool_addresses.extend([p['id'] for p in pools])\n",
    "        if len(pools) < first:\n",
    "            break\n",
    "        skip += first\n",
    "    return pool_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2c278",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bb2c278",
    "outputId": "8318e620-8bab-4d5f-9de1-b4afc79c9292"
   },
   "outputs": [],
   "source": [
    "# Define training and test pool addresses\n",
    "api_key = \"d1762c97d76a973e078c5536742bd237\"  # Replace with your API key\n",
    "subgraph_id = \"5zvR82QoaXYFyDEKLZ9t6v9adgnptxYpKpSbxtgVENFV\"    # Replace with your subgraph id\n",
    "\n",
    "start_date = \"2024-06-01\"\n",
    "end_date = \"2025-06-01\"\n",
    "train_end_date = \"2025-04-01\"  \n",
    "\n",
    "# Pool address to test on (should be in the same format)\n",
    "main_pool_address = \"0xcbcdf9626bc03e24f779434178a73a0b4bad62ed\"  # Example test pool\n",
    "\n",
    "# Pools to pretrain on\n",
    "# pre_train_pool_addresses = [\n",
    "#     \"0x88e6A0c2dDD26FEEb64F039a2c41296FcB3f5640\", # USDC/ETH\n",
    "#     \"0x99ac8cA7087fA4A2A1FB6357269965A2014ABc35\", # WBTC/USDC\n",
    "#     \"0xe8f7c89C5eFa061e340f2d2F206EC78FD8f7e124\", # WBTC/cbBTC\n",
    "#     \"0x5777d92f208679DB4b9778590Fa3CAB3aC9e2168\", # DAI/USDC\n",
    "#     \"0x4e68Ccd3E89f51C3074ca5072bbAC773960dFa36\", # ETH/USDT\n",
    "#     \"0x3416cF6C708Da44DB2624D63ea0AAef7113527C6\", # USDC/USDT\n",
    "#     \"0xC5c134A1f112efA96003f8559Dba6fAC0BA77692\", # WHITE/ETH\n",
    "#     \"0x8ad599c3A0ff1De082011EFDDc58f1908eb6e6D8\", # USDC/ETH\n",
    "#     \"0x9Db9e0e53058C89e5B94e29621a205198648425B\", # WBTC/USDT\n",
    "#     \"0x87428a53e14d24Ab19c6Ca4939B4df93B8996cA9\"\n",
    "# ]\n",
    "pre_train_pool_addresses = fetch_all_pool_addresses(api_key, subgraph_id)\n",
    "pre_train_pool_addresses = [addr.lower() for addr in pre_train_pool_addresses]  # Ensure all addresses are in lowercase\n",
    "random.shuffle(pre_train_pool_addresses)  # Shuffle the pre-train pools for randomness\n",
    "# Limit the number of pre-train pools to a reasonable size\n",
    "pre_train_pool_addresses = pre_train_pool_addresses[:1000]  # Adjust this number as needed\n",
    "\n",
    "# --- Fetch and cache all pool data at the start ---\n",
    "pool_addresses = [main_pool_address] + pre_train_pool_addresses\n",
    "\n",
    "# Fetch and store all pool data in a dictionary\n",
    "pool_data_dict = {}\n",
    "print(f\"Fetching data for main pool: {main_pool_address}\")\n",
    "df = fetch_pool_hourly_data(api_key, subgraph_id, main_pool_address, start_date, end_date)\n",
    "print(f\"Data for main pool fetched, shape: {df.shape}\")\n",
    "pool_data_dict[main_pool_address] = df\n",
    "for pool_addr in pre_train_pool_addresses:\n",
    "    print(f\"Fetching data for pool: {pool_addr}\")\n",
    "    df_pool = fetch_pool_hourly_data(api_key, subgraph_id, pool_addr, start_date, train_end_date)\n",
    "    # Check if DataFrame is not empty and has the same columns as df\n",
    "    if len(df_pool)>100 and set(df_pool.columns) == set(df.columns):\n",
    "        print(f\"Data for pool {pool_addr} fetched, shape: {df_pool.shape}\")\n",
    "        pool_data_dict[pool_addr] = df_pool\n",
    "    else:\n",
    "        print(f\"Skipping pool {pool_addr}: empty or columns mismatch (columns: {df_pool.columns.tolist()})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37205312",
   "metadata": {
    "id": "37205312"
   },
   "source": [
    "## Feature Engineering: Price Returns, Volatility, Moving Averages, and Time Features\n",
    "\n",
    "In this section, we compute engineered features for the PLV model, including:\n",
    "- Price returns (log and simple)\n",
    "- Rolling volatility (e.g., 6-hour standard deviation)\n",
    "- Moving averages (e.g., 6-hour, 24-hour)\n",
    "- Time-based features (hour of day, day of week, month, season)\n",
    "\n",
    "These features are useful for modeling and backtesting strategies on Uniswap V3 pool data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7443a4a",
   "metadata": {
    "id": "d7443a4a"
   },
   "outputs": [],
   "source": [
    "# Feature engineering function (reuse from earlier)\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    if 'datetime' not in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df['periodStartUnix'], unit='s')\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "    # Returns\n",
    "    df['price_return'] = df['price'].pct_change()\n",
    "    df['liquidity_return'] = df['liquidity'].pct_change()\n",
    "    df['volume_return'] = df['volumeUSD'].pct_change()\n",
    "\n",
    "    # Outlier removal\n",
    "    def remove_outliers_iqr(series, k=3.0):\n",
    "        q_01 = series.quantile(0.3)\n",
    "        q_90 = series.quantile(0.6)\n",
    "        iqr = q_90 - q_01\n",
    "        lower = q_01 - k * iqr\n",
    "        upper = q_90 + k * iqr\n",
    "        return series.where((series >= lower) & (series <= upper))\n",
    "\n",
    "    for col in ['price_return', 'liquidity_return', 'volume_return']:\n",
    "        if col in df.columns:\n",
    "            df[col] = remove_outliers_iqr(df[col])\n",
    "            df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Price-based volatility and moving averages\n",
    "    df['price_volatility_3h'] = df['price_return'].rolling(window=3).std()\n",
    "    df['price_volatility_6h'] = df['price_return'].rolling(window=6).std()\n",
    "    df['price_volatility_24h'] = df['price_return'].rolling(window=24).std()\n",
    "    df['price_ma_3h'] = df['price_return'].rolling(window=3).mean()\n",
    "    df['price_ma_6h'] = df['price_return'].rolling(window=6).mean()\n",
    "    df['price_ma_24h'] = df['price_return'].rolling(window=24).mean()\n",
    "\n",
    "    # Liquidity-based volatility and moving averages (shifted by 1 to avoid lookahead bias)\n",
    "    df['liquidity_volatility_3h'] = df['liquidity_return'].shift(1).rolling(window=3).std()\n",
    "    df['liquidity_volatility_6h'] = df['liquidity_return'].shift(1).rolling(window=6).std()\n",
    "    df['liquidity_volatility_24h'] = df['liquidity_return'].shift(1).rolling(window=24).std()\n",
    "    df['liquidity_ma_3h'] = df['liquidity_return'].shift(1).rolling(window=3).mean()\n",
    "    df['liquidity_ma_6h'] = df['liquidity_return'].shift(1).rolling(window=6).mean()\n",
    "    df['liquidity_ma_24h'] = df['liquidity_return'].shift(1).rolling(window=24).mean()\n",
    "\n",
    "    # Temporal features\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 0  # Winter\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 1  # Spring\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 2  # Summer\n",
    "        else:\n",
    "            return 3  # Fall\n",
    "\n",
    "    df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "    if 'periodStartUnix' in df.columns:\n",
    "        df = df.drop(columns=['periodStartUnix'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Prepare lagged features for a given DataFrame\n",
    "def add_lagged_features(df, n_lags=3):\n",
    "    df = df.copy()\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        df[f'price_return_lag{lag}'] = df['price_return'].shift(lag)\n",
    "        df[f'liquidity_return_lag{lag}'] = df['liquidity_return'].shift(lag)\n",
    "    return df\n",
    "\n",
    "df_fe = feature_engineering(df)\n",
    "df_fe = add_lagged_features(df_fe, n_lags=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7495a5a",
   "metadata": {
    "id": "d7495a5a"
   },
   "source": [
    "## Analytical Plots: Feature Exploration\n",
    "\n",
    "Let's visualize the main features and their relationships to better understand the data and engineered features. We'll plot time series, histograms, and scatter plots for key variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ed728",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c72ed728",
    "outputId": "209264e8-adde-4a05-cbaa-c04c1869375e"
   },
   "outputs": [],
   "source": [
    "# Plot raw price over time\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df_fe['datetime'], df_fe['price'], label='Price', color='tab:blue')\n",
    "plt.title('Raw Price Over Time')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot price return over time\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df_fe['datetime'], df_fe['price_return'], label='Price Return', color='tab:green')\n",
    "plt.title('Price Return Over Time')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Price Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot liquidity return over time\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df_fe['datetime'], df_fe['liquidity_return'], label='Liquidity Return', color='tab:orange')\n",
    "plt.title('Liquidity Return Over Time')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Liquidity Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot volume return over time\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df_fe['datetime'], df_fe['volume_return'], label='Volume Return', color='tab:red')\n",
    "plt.title('Volume Return Over Time')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Volume Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Histograms of returns\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "sns.histplot(df_fe['price_return'].dropna(), bins=50, ax=axs[0], kde=True)\n",
    "axs[0].set_title('Price Return Distribution')\n",
    "sns.histplot(df_fe['liquidity_return'].dropna(), bins=50, ax=axs[1], kde=True)\n",
    "axs[1].set_title('Liquidity Return Distribution')\n",
    "sns.histplot(df_fe['volume_return'].dropna(), bins=50, ax=axs[2], kde=True)\n",
    "axs[2].set_title('Volume Return Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: price_return vs liquidity_return\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(x=df_fe['price_return'], y=df_fe['liquidity_return'], alpha=0.5)\n",
    "plt.title('Scatter: Price Return vs Liquidity Return')\n",
    "plt.xlabel('Price Return')\n",
    "plt.ylabel('Liquidity Return')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap of engineered features\n",
    "features = [\n",
    "    'liquidity_return', 'liquidity_return_lag1', \n",
    "    'price_return', 'price_return_lag1', \n",
    "    'volume_return',\n",
    "    'price_volatility_3h', 'price_volatility_6h', 'price_volatility_24h',\n",
    "    'liquidity_volatility_3h', 'liquidity_volatility_6h', 'liquidity_volatility_24h',\n",
    "    'price_ma_3h', 'price_ma_6h', 'price_ma_24h',\n",
    "    'liquidity_ma_3h', 'liquidity_ma_6h', 'liquidity_ma_24h',\n",
    "    'hour', 'day_of_week', 'month', 'season'\n",
    "]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_fe[features].corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Engineered Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e95a40",
   "metadata": {
    "id": "c3e95a40"
   },
   "source": [
    "## Benchmark Model and Evaluation Functions\n",
    "\n",
    "We start with a naive benchmark model that predicts the next value as the current value (a persistence or \"random walk\" model). We also define evaluation functions (e.g., MAE, RMSE, MAPE) to assess model performance. These functions can be reused for more sophisticated models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a51a4d",
   "metadata": {
    "id": "00a51a4d"
   },
   "outputs": [],
   "source": [
    "# Prepare training data: split chronologically by train_end_date (date-based)\n",
    "def get_chronological_split(df, train_end_date):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into train and test sets based on a train_end_date (string or datetime).\n",
    "    All rows with datetime <= train_end_date go to train, the rest to test.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    if isinstance(train_end_date, str):\n",
    "        train_end_date = pd.to_datetime(train_end_date)\n",
    "    train_df = df[df['datetime'] <= train_end_date].reset_index(drop=True)\n",
    "    test_df = df[df['datetime'] > train_end_date].reset_index(drop=True)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dfb74",
   "metadata": {
    "id": "676dfb74"
   },
   "outputs": [],
   "source": [
    "def naive_predict(series):\n",
    "    \"\"\"Naive model: predicts next value as the current value (persistence).\"\"\"\n",
    "    # Shift series by 1 to get prediction for t+1 at time t\n",
    "    return series.shift(1)\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"Compute MAE, RMSE, and MAPE for predictions.\"\"\"\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100 if np.all(y_true != 0) else np.nan\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, title=\"Actual vs Predicted Liquidity Return\"):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    # If input is a numpy array, use np.arange for x-axis\n",
    "    if hasattr(y_true, 'index'):\n",
    "        x = y_true.index\n",
    "    else:\n",
    "        x = np.arange(len(y_true))\n",
    "    plt.plot(x, y_true, label=\"Actual\", color=\"tab:blue\")\n",
    "    plt.plot(x, y_pred, label=\"Predicted\", color=\"tab:orange\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Liquidity Return\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bb264",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "909bb264",
    "outputId": "c05ea9fe-b477-42be-c006-24b30ef5ddd3"
   },
   "outputs": [],
   "source": [
    "# Example: Benchmark for price prediction\n",
    "test_split = 0.9\n",
    "_, test_df = get_chronological_split(df_fe, train_end_date)\n",
    "y_true = test_df['liquidity_return'].iloc[1:]  # true values from t=1\n",
    "naive_pred = naive_predict(test_df['liquidity_return']).iloc[1:]  # predictions for t=1 onward\n",
    "results = evaluate_predictions(y_true, naive_pred)\n",
    "print(\"Naive benchmark results for liquidity_return:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79ff57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "7f79ff57",
    "outputId": "6928d8f7-d2ec-4586-e681-b2b9dcc88d43"
   },
   "outputs": [],
   "source": [
    "# Plot for Naive Benchmark Model\n",
    "plot_actual_vs_predicted(y_true, naive_pred, title=\"Naive Model: Actual vs Predicted Liquidity Return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ff11c",
   "metadata": {},
   "source": [
    "## Zero-inflated LSTM Regression: Multi-Pool Pretraining and Fine-Tuning\n",
    "\n",
    "We now use a zero-inflated LSTM model to predict liquidity returns from price and engineered features. The model is first pretrained on multiple pools, then fine-tuned on a specific pool. Both the classifier (zero/nonzero) and regressor (nonzero) use LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ZeroInflatedLSTM class for zero-inflated modeling (PyTorch version, with internal scaling) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ZeroInflatedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, n_lags=5, lstm_units=32, dense_units=16):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, lstm_units, batch_first=True)\n",
    "        self.shared_dense = nn.Linear(lstm_units, dense_units)\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Linear(dense_units, 1)\n",
    "        # Regressor head\n",
    "        self.regressor_dense = nn.Linear(dense_units, dense_units)\n",
    "        self.regressor = nn.Linear(dense_units, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.feature_scaler = None\n",
    "        self.target_reg_scaler = None\n",
    "        self._input_size = input_size\n",
    "        self.n_lags = n_lags\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]\n",
    "        x = self.relu(self.shared_dense(x))\n",
    "        cls_out = self.sigmoid(self.classifier(x))\n",
    "        reg_x = self.relu(self.regressor_dense(x))\n",
    "        reg_out = self.regressor(reg_x)\n",
    "        return cls_out, reg_out\n",
    "\n",
    "    def fit(self, X, y_cls, y_reg, epochs=20, batch_size=64, lr=0.001, val_data=None, verbose=1):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(device)\n",
    "        # Fit scalers on training data\n",
    "        n_samples, n_lags, n_features = X.shape\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        X_reshaped = X.reshape(-1, n_features)\n",
    "        self.feature_scaler.fit(X_reshaped)\n",
    "        X_scaled = self.feature_scaler.transform(X_reshaped).reshape(n_samples, n_lags, n_features)\n",
    "        self.target_reg_scaler = StandardScaler()\n",
    "        self.target_reg_scaler.fit(y_reg.reshape(-1, 1))\n",
    "        y_reg_scaled = self.target_reg_scaler.transform(y_reg.reshape(-1, 1)).flatten()\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32, device=device)\n",
    "        y_cls_tensor = torch.tensor(y_cls, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        y_reg_tensor = torch.tensor(y_reg_scaled, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        n = X_tensor.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            perm = torch.randperm(n)\n",
    "            total_loss = 0\n",
    "            for i in range(0, n, batch_size):\n",
    "                idx = perm[i:i+batch_size]\n",
    "                xb, ycb, yrb = X_tensor[idx], y_cls_tensor[idx], y_reg_tensor[idx]\n",
    "                optimizer.zero_grad()\n",
    "                cls_pred, reg_pred = self(xb)\n",
    "                loss = custom_zi_loss(cls_pred, reg_pred, ycb, yrb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * len(idx)\n",
    "            if verbose and (epoch % 5 == 0 or epoch == epochs-1):\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/n:.4f}\")\n",
    "                if val_data is not None:\n",
    "                    val_loss = self.evaluate(*val_data)\n",
    "                    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X, y_cls, y_reg):\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            n_samples, n_lags, n_features = X.shape\n",
    "            X_scaled = self.feature_scaler.transform(X.reshape(-1, n_features)).reshape(n_samples, n_lags, n_features)\n",
    "            y_reg_scaled = self.target_reg_scaler.transform(y_reg.reshape(-1, 1)).flatten()\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32, device=device)\n",
    "            y_cls_tensor = torch.tensor(y_cls, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            y_reg_tensor = torch.tensor(y_reg_scaled, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            cls_pred, reg_pred = self(X_tensor)\n",
    "            loss = custom_zi_loss(cls_pred, reg_pred, y_cls_tensor, y_reg_tensor)\n",
    "        return loss.item()\n",
    "\n",
    "    def predict(self, X):\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            n_samples, n_lags, n_features = X.shape\n",
    "            X_scaled = self.feature_scaler.transform(X.reshape(-1, n_features)).reshape(n_samples, n_lags, n_features)\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32, device=device)\n",
    "            cls_pred, reg_pred = self(X_tensor)\n",
    "            cls_pred = (cls_pred.cpu().numpy().flatten() > 0.5).astype(int)\n",
    "            reg_pred = reg_pred.cpu().numpy().flatten()\n",
    "            # Inverse transform regression output\n",
    "            reg_pred = self.target_reg_scaler.inverse_transform(reg_pred.reshape(-1, 1)).flatten()\n",
    "        return reg_pred, cls_pred\n",
    "\n",
    "    def get_X_y(self, df, features, target_col):\n",
    "        X, y_cls, y_reg = [], [], []\n",
    "        for i in range(self.n_lags, len(df)):\n",
    "            X.append(df[features].iloc[i-self.n_lags:i].values)\n",
    "            target = df[target_col].iloc[i]\n",
    "            y_cls.append(1 if target == 0 else 0)\n",
    "            y_reg.append(target)\n",
    "        return np.array(X), np.array(y_cls), np.array(y_reg)\n",
    "\n",
    "    def recursive_forecast(self, df, features, forecast_horizon=20, verbose=False):\n",
    "        \"\"\"\n",
    "        Recursive multi-step forecast, updating lagged and rolling features.\n",
    "        df: DataFrame with all features (including price/time/rolling features)\n",
    "        features: list of feature columns to use for prediction\n",
    "        forecast_horizon: number of steps to forecast\n",
    "        Returns: list of predicted liquidity_return values\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        reg_preds = []\n",
    "        cls_preds = []\n",
    "        history = df.iloc[:-forecast_horizon].copy()\n",
    "\n",
    "        for step in range(forecast_horizon):\n",
    "            next_idx = history.index[-1] + 1\n",
    "\n",
    "            # Compose next row's features\n",
    "            next_row = self._update_rolling_features(df, next_idx, features, history)\n",
    "            if verbose:\n",
    "                print(f\"Step {step + 1}/{forecast_horizon}, Predicting for index {next_idx} with features: {next_row}\")\n",
    "\n",
    "            # Prepare input sequence for LSTM\n",
    "            X_seq = history[features].iloc[-self.n_lags:].values\n",
    "            X_seq = X_seq.reshape(1, self.n_lags, len(features))\n",
    "            reg_pred, cls_pred = self.predict(X_seq)\n",
    "            reg_pred = reg_pred[0]\n",
    "            cls_pred = cls_pred[0]\n",
    "            pred_val = reg_pred if cls_pred == 0 else 0.0\n",
    "            reg_preds.append(reg_pred)\n",
    "            cls_preds.append(cls_pred)\n",
    "            # Update history with new prediction\n",
    "            new_row = df.iloc[len(history)] if len(history) < len(df) else history.iloc[-1].copy()\n",
    "            new_row = new_row.copy()\n",
    "            new_row['liquidity_return'] = pred_val\n",
    "            history = pd.concat([history, pd.DataFrame([new_row], index=[next_idx])])\n",
    "\n",
    "        return np.array(reg_preds), np.array(cls_preds)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_rolling_features(df, next_idx, features, history):\n",
    "        \"\"\"\n",
    "        Helper to compute next row's lagged and rolling features for recursive forecasting.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        next_row = {}\n",
    "\n",
    "        for col in features:\n",
    "            if 'liquidity' not in col:\n",
    "                # Non-liquidity features: take from df if available\n",
    "                next_row[col] = df.loc[next_idx, col] if next_idx in df.index else np.nan\n",
    "                continue\n",
    "\n",
    "            # Liquidity-based features\n",
    "            if col.endswith('return'):\n",
    "                next_row[col] = history[col].iloc[-1] if not history[col].empty else np.nan\n",
    "            elif 'volatility' in col:\n",
    "                try:\n",
    "                    win = int(col.split('_')[-1][:-1])\n",
    "                    next_row[col] = history['liquidity_return'].iloc[-win:].std()\n",
    "                except Exception:\n",
    "                    next_row[col] = np.nan\n",
    "            elif 'ma' in col:\n",
    "                try:\n",
    "                    win = int(col.split('_')[-1][:-1])\n",
    "                    next_row[col] = history['liquidity_return'].iloc[-win:].mean()\n",
    "                except Exception:\n",
    "                    next_row[col] = np.nan\n",
    "            elif 'lag' in col:\n",
    "                try:\n",
    "                    lag_num = int(col.split('lag')[-1])\n",
    "                    next_row[col] = history[col].iloc[-lag_num] if len(history) >= lag_num else np.nan\n",
    "                except Exception:\n",
    "                    next_row[col] = np.nan\n",
    "            else:\n",
    "                next_row[col] = np.nan  # fallback for unexpected patterns\n",
    "\n",
    "        return next_row\n",
    "\n",
    "def custom_zi_loss(cls_pred, reg_pred, y_cls, y_reg):\n",
    "    bce = nn.BCELoss()(cls_pred, y_cls)\n",
    "    mask = (y_cls == 0).float()\n",
    "    if mask.sum() > 0:\n",
    "        mse = ((reg_pred.squeeze() - y_reg.squeeze()) ** 2 * mask).sum() / (mask.sum() + 1e-6)\n",
    "    else:\n",
    "        mse = torch.tensor(0.0, device=reg_pred.device)\n",
    "    return bce + mse\n",
    "\n",
    "def make_lagged_features(base, n_lags):\n",
    "    return [f\"{base}_lag{lag}\" for lag in range(1, n_lags+1)]\n",
    "\n",
    "def dropna_lstm(df, features, target_col):\n",
    "    mask = df[features + [target_col]].notnull().all(axis=1)\n",
    "    return df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "def prepare_lstm_pretrain_data(pool_data_dict, lstm_model, lstm_features, target, train_end_date):\n",
    "    X_pre_list, y_cls_pre_list, y_reg_pre_list = [], [], []\n",
    "    for pool_addr, df_pool in pool_data_dict.items():\n",
    "        df_pool = df_pool.copy()\n",
    "        df_pool = feature_engineering(df_pool)\n",
    "        df_pool = df_pool[df_pool['datetime'] <= pd.to_datetime(train_end_date)].copy()\n",
    "        df_pool = add_lagged_features(df_pool, n_lags=lstm_model.n_lags)\n",
    "        df_pool['pool'] = pool_addr\n",
    "        df_pool = dropna_lstm(df_pool, lstm_features, target)\n",
    "        Xp, yc, yr = lstm_model.get_X_y(df_pool, lstm_features, target)\n",
    "        X_pre_list.append(Xp)\n",
    "        y_cls_pre_list.append(yc)\n",
    "        y_reg_pre_list.append(yr)\n",
    "    X_pre = np.concatenate(X_pre_list, axis=0)\n",
    "    y_cls_pre = np.concatenate(y_cls_pre_list, axis=0)\n",
    "    y_reg_pre = np.concatenate(y_reg_pre_list, axis=0)\n",
    "    return X_pre, y_cls_pre, y_reg_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b44976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare data for LSTM: multi-pool pretraining and fine-tuning ---\n",
    "# n_lags will be set per model instance\n",
    "# Example default for initial setup\n",
    "n_lags_default = 5\n",
    "\n",
    "lstm_features = [\n",
    "    'price_return', 'price_volatility_3h', 'price_volatility_6h', 'price_volatility_24h',\n",
    "    'liquidity_volatility_3h', 'liquidity_volatility_6h', 'liquidity_volatility_24h',\n",
    "    'price_ma_3h', 'price_ma_6h', 'price_ma_24h',\n",
    "    'liquidity_ma_3h', 'liquidity_ma_6h', 'liquidity_ma_24h',\n",
    "    'hour', 'day_of_week', 'month', 'season',\n",
    "] + make_lagged_features('price_return', n_lags_default) + make_lagged_features('liquidity_return', n_lags_default)\n",
    "\n",
    "target = 'liquidity_return'\n",
    "\n",
    "# Instantiate model with n_lags_default for initial data prep\n",
    "input_size = len(lstm_features)\n",
    "lstm_model = ZeroInflatedLSTM(input_size, n_lags=n_lags_default, lstm_units=32, dense_units=16)\n",
    "\n",
    "X_pre, y_cls_pre, y_reg_pre = prepare_lstm_pretrain_data(\n",
    "    pool_data_dict, lstm_model, lstm_features, target, train_end_date\n",
    ")\n",
    "\n",
    "df_main = pool_data_dict[main_pool_address].copy()\n",
    "df_main = feature_engineering(df_main)\n",
    "df_main = add_lagged_features(df_main, n_lags=lstm_model.n_lags)\n",
    "df_main['pool'] = main_pool_address\n",
    "df_main = dropna_lstm(df_main, lstm_features, target)\n",
    "df_finetune, df_test = get_chronological_split(df_main, train_end_date)\n",
    "X_fine, y_cls_fine, y_reg_fine = lstm_model.get_X_y(df_finetune, lstm_features, target)\n",
    "\n",
    "print(f\"Pretraining set shape: {X_pre.shape}, {y_cls_pre.shape}, {y_reg_pre.shape}\")\n",
    "print(f\"Fine-tuning set shape: {X_fine.shape}, {y_cls_fine.shape}, {y_reg_fine.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train and fine-tune ZeroInflatedLSTM ---\n",
    "\n",
    "# Pretrain on all pools\n",
    "print(\"Pretraining on multiple pools...\")\n",
    "lstm_model.fit(X_pre, y_cls_pre, y_reg_pre, epochs=20, batch_size=64, lr=0.001, verbose=1)\n",
    "# The idea is to train this model on multiple pools to learn general patterns in liquidity and price movements.\n",
    "# Then, the model is saved and can be loaded for fine-tuning on a specific pool.\n",
    "\n",
    "# Fine-tune on specific pool\n",
    "print(f\"Fine-tuning on pool: {main_pool_address}...\")\n",
    "lstm_model.fit(X_fine, y_cls_fine, y_reg_fine, epochs=20, batch_size=32, lr=0.001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd893c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "X_test, y_cls_test, y_reg_test = lstm_model.get_X_y(df_test, lstm_features, target)\n",
    "print(f\"Test set shape: {X_test.shape}, {y_cls_test.shape}, {y_reg_test.shape}\")\n",
    "y_reg_pred, y_cls_pred = lstm_model.predict(X_test)\n",
    "\n",
    "mask = y_cls_test == 0\n",
    "reg_rmse = mean_squared_error(y_reg_test[mask], y_reg_pred[mask])\n",
    "cls_acc = accuracy_score(y_cls_test, y_cls_pred)\n",
    "print(f\"Classifier accuracy: {cls_acc}\")\n",
    "print(f\"Conditional regression RMSE (nonzero): {reg_rmse}\")\n",
    "cm = confusion_matrix(y_cls_test, y_cls_pred)\n",
    "print(\"Confusion matrix (rows: true, cols: pred):\\n\", cm)\n",
    "\n",
    "y_reg_test_masked = np.zeros(len(y_reg_test))\n",
    "y_reg_test_masked[mask] = y_reg_test[mask]\n",
    "y_reg_pred_masked = np.zeros(len(y_reg_pred))\n",
    "y_reg_pred_masked[mask] = y_reg_pred[mask]\n",
    "plot_actual_vs_predicted(y_reg_test_masked, y_reg_pred_masked, title=\"LSTM: Actual vs Predicted Liquidity Return (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06b0f8",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning and Autoregressive Forecasting\n",
    "\n",
    "We perform hyperparameter tuning for the LSTM model using grid search. This automated process tests different combinations of hyperparameters (e.g., LSTM units, dense units, learning rate, batch size) to find the best model configuration. Then, we use the best model to perform autoregressive forecasting, effectively simulating a deployment scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid search for LSTM hyperparameters ---\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def grid_search_lstm(\n",
    "    pool_data_dict,\n",
    "    base_features,\n",
    "    target_col,\n",
    "    train_end_date,\n",
    "    param_grid,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Automated grid search for ZeroInflatedLSTM.\n",
    "    - pool_data_dict: dict of pool dataframes\n",
    "    - base_features: list of feature names excluding lags\n",
    "    - target_col: name of target column\n",
    "    - train_end_date: date for splitting data\n",
    "    - param_grid: dict of hyperparameters (lstm_units, dense_units, batch_size, n_lags, etc.)\n",
    "    - validation_split: fraction of data for validation\n",
    "    - epochs: training epochs\n",
    "    Returns best model, best params, and results list.\n",
    "    \"\"\"\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    results = []\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        n_lags = params.get('n_lags', 5)\n",
    "        # Dynamically generate lagged features for this n_lags\n",
    "        feature_list = base_features + make_lagged_features('price_return', n_lags) + make_lagged_features('liquidity_return', n_lags)\n",
    "        model = ZeroInflatedLSTM(\n",
    "            input_size=len(feature_list),\n",
    "            n_lags=n_lags,\n",
    "            lstm_units=params.get('lstm_units', 32),\n",
    "            dense_units=params.get('dense_units', 16)\n",
    "        )\n",
    "        X, y_cls, y_reg = prepare_lstm_pretrain_data(pool_data_dict, model, feature_list, target_col, train_end_date)\n",
    "        split_idx = int(len(X) * (1 - validation_split))\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_cls_train, y_cls_val = y_cls[:split_idx], y_cls[split_idx:]\n",
    "        y_reg_train, y_reg_val = y_reg[:split_idx], y_reg[split_idx:]\n",
    "        model.fit(\n",
    "            X_train, y_cls_train, y_reg_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=params.get('batch_size', 64),\n",
    "            verbose=0\n",
    "        )\n",
    "        reg_pred, cls_pred = model.predict(X_val)\n",
    "        mask_val = y_cls_val == 0\n",
    "        rmse = mean_squared_error(y_reg_val[mask_val], reg_pred[mask_val])\n",
    "        results.append((params, rmse))\n",
    "        if verbose:\n",
    "            print(f\"Params: {params}, Val RMSE: {rmse}\")\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "    if verbose:\n",
    "        print(f\"\\nBest params: {best_params}, Best Val RMSE: {best_score}\")\n",
    "    return best_model, best_params, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a95362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example grid search usage ---\n",
    "param_grid = {\n",
    "    'lstm_units': [16, 32],\n",
    "    'dense_units': [8, 16],\n",
    "    'n_lags': [3, 5],\n",
    "    'batch_size': [32]\n",
    "}\n",
    "\n",
    "base_features = [\n",
    "    'price_return', 'price_volatility_3h', 'price_volatility_6h', 'price_volatility_24h',\n",
    "    'liquidity_volatility_3h', 'liquidity_volatility_6h', 'liquidity_volatility_24h',\n",
    "    'price_ma_3h', 'price_ma_6h', 'price_ma_24h',\n",
    "    'liquidity_ma_3h', 'liquidity_ma_6h', 'liquidity_ma_24h',\n",
    "    'hour', 'day_of_week', 'month', 'season',\n",
    "]\n",
    "\n",
    "target_col = 'liquidity_return'\n",
    "\n",
    "# Perform grid search\n",
    "best_model, best_params, search_results = grid_search_lstm(\n",
    "    pool_data_dict=pool_data_dict,\n",
    "    base_features=base_features,\n",
    "    target_col=target_col,\n",
    "    train_end_date=train_end_date,\n",
    "    param_grid=param_grid,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", best_params)\n",
    "# Plot search results\n",
    "results_df = pd.DataFrame(search_results, columns=['params', 'rmse'])\n",
    "# Convert params dicts to strings for plotting\n",
    "results_df['params_str'] = results_df['params'].apply(lambda d: str(d))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='params_str', y='rmse', data=results_df)\n",
    "plt.title(\"Grid Search Results: RMSE by Hyperparameter Combination\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model for final predictions\n",
    "best_n_lags = best_params.get('n_lags', 5)\n",
    "best_lstm_features = base_features + make_lagged_features('price_return', best_n_lags) + make_lagged_features('liquidity_return', best_n_lags)\n",
    "best_input_size = len(best_lstm_features)\n",
    "\n",
    "# Fine-tune on main pool\n",
    "df_main_finetune, _ = get_chronological_split(df_main, train_end_date)\n",
    "X_fine, y_cls_fine, y_reg_fine = best_model.get_X_y(df_main_finetune, best_lstm_features, target_col)\n",
    "best_model.fit(X_fine, y_cls_fine, y_reg_fine, epochs=20, batch_size=best_params.get('batch_size', 64), verbose=1)\n",
    "\n",
    "# Recursive forecast on test set\n",
    "forecast_horizon = len(df_test) - best_n_lags\n",
    "y_reg_pred, y_cls_pred = best_model.recursive_forecast(df_main, best_lstm_features, forecast_horizon=forecast_horizon, verbose=True)\n",
    "\n",
    "# Evaluate\n",
    "X_test, y_cls_test, y_reg_test = best_model.get_X_y(df_test, best_lstm_features, target_col)\n",
    "mask = y_cls_test == 0\n",
    "reg_rmse = mean_squared_error(y_reg_test[mask], y_reg_pred[mask])\n",
    "cls_acc = accuracy_score(y_cls_test, y_cls_pred)\n",
    "print(f\"Classifier accuracy: {cls_acc}\")\n",
    "print(f\"Conditional regression RMSE (nonzero): {reg_rmse}\")\n",
    "cm = confusion_matrix(y_cls_test, y_cls_pred)\n",
    "print(\"Confusion matrix (rows: true, cols: pred):\\n\", cm)\n",
    "\n",
    "y_reg_test_masked = np.zeros(len(y_reg_test))\n",
    "y_reg_test_masked[mask] = y_reg_test[mask]\n",
    "y_reg_pred_masked = np.zeros(len(y_reg_pred))\n",
    "y_reg_pred_masked[mask] = y_reg_pred[mask]\n",
    "plot_actual_vs_predicted(y_reg_test_masked, y_reg_pred_masked, title=\"LSTM Recursive Forecast: Actual vs Predicted Liquidity Return\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
